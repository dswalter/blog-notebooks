{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Intro:\n",
    "no python code\n",
    "Probabilistic counting and loglog\n",
    "Hyperloglog++ explanation \n",
    "and python code\n",
    "References. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Getting to HyperLogLog\n",
    "\n",
    "### Introduction: what problem are we solvings?\n",
    "\n",
    "HyperLogLog is an algorithm for approximating cardinality, a mathematical term for the number of unique elements in a group.  We ask questions about cardinality, sometimes called the COUNT DISTINCT problem, all the time. Maybe you care about the number of unique comments submitted to the FCC in [defense of net neutrality](https://arstechnica.com/tech-policy/2017/08/isp-funded-study-finds-huge-support-for-keeping-current-net-neutrality-rules/), or you wanted to know how many unique words were present in the first stanza of Jabberwocky, or maybe you’re just curious about the number of unique visitors to your boutique puppy hair salon. \n",
    "\n",
    "The simplest way to do this is to use a data structure with quick membership checks, like a hash table. Check if every entry in your dataset is in the dictionary; whenever it’s not, insert that element and increment your counter. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 23 elements in our original dataset \n",
      "We have 18 distinct elements in this dataset.\n"
     ]
    }
   ],
   "source": [
    "jabber_string = ['twas','brillig','and','the','slithy','toves','did','gyre',\n",
    "'and','gimble','in','the','wabe','all','mimsy','were','the','borogoves','and',\n",
    "'the','mome','raths','outgrabe']\n",
    "\n",
    "print(f\"We have {len(jabber_string)} elements in our original dataset \")\n",
    "\n",
    "holder_dictionary = dict()\n",
    "distinct_count = 0\n",
    "for element in jabber_string:\n",
    "    if element not in holder_dictionary:   ##All-important membership check\n",
    "        distinct_count+=1                  ##deeply un-pythonic way of doing this. \n",
    "        holder_dictionary[element]='present'\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(f\"We have {distinct_count} distinct elements in this dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is all well and good for a dataset with 23 elements. But if you have billions of visitors to your doggy spa page, this calculation becomes memory-hungry and computationally expensive. You could distribute the work to many worker nodes, but then you have distributed computing problems: you have to reconcile the results of each node’s computations and there’s a lot of data to pass back from each node. Using a map-reduce approach, even if you reduce locally on your worker nodes, you’ll still have to send back to the master node an array of values present in your reducer. \n",
    "\n",
    "An important question in solving this problem is: do you need to know _exactly_ how many visitors came to your site? If we are freed from the tyranny of the exact, our lives are much simpler. If we can put up with a close approximation of the number of distinct elements, we can get substantial memory and computational gains using algorithms from the field of Probabilistic Counting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## A Quick History of Probabilistic Counting.\n",
    "\n",
    "#### A formal origin\n",
    "For anyone with an inconvenient amount of data, approximate inference is a fairly common tool; you can stomach a reduction in accuracy for a reduction in execution time or computer memory required. At a large enough scale scale, exact inference on some problems would take millions of years to complete. But the application of probabilistic methods to *counting itself* is a relatively recent phenomenon. In 1978, Robert Morris at Bell Labs introduced a method for producing an estimated count from dataset.  Motivated by how the number of heads found by flipping an unbiased coin would produce roughly half the total number of coin flips, his method provided a way to specify the amount of error allowed for a given computational memory budget. This idea had clear industrial benefits, especially in memory-constrained applications like embedded systems. \n",
    "\n",
    "#### Philippe Flajolet\n",
    "The idea of approximate counting appears to have jammed itself into the mind of French Computer Scientist/Mathematician Philippe Flajolet, who in 1985 wrote [a more extended treatment](http://algo.inria.fr/flajolet/Publications/Flajolet85c.pdf) of Morris’ approximate counting algorithm. Soon afterward, he published a paper [introducing a probabilistic method](http://algo.inria.fr/flajolet/Publications/FlMa85.pdf) for the count distinct problem. \n",
    "\n",
    "\n",
    "In his algorithm, Probabilistic Counting with Stochastic Averaging (PCSA), he used the idea that  based on the idea that, given a uniform hash function, the position of the leftmost zero in the hash of the object to be counted is an approximation of logsub2 of n. Calling that R, he scaled lambda(n) = (1/phi)2^^Rsubn. The approximation was too inexact, so he proposed using stochastic averaging on the R’s. While hashing, group the records into M groups by using hash(input) mod(M), then calculating the Rsubn for that group. Finally, aggregate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lsb = -1\n",
    "ones_found = [\"0\" for i in range(128)]\n",
    "for element in jabber_string:\n",
    "    m = md5()\n",
    "    m.update(element.encode())\n",
    "    hashcode = m.hexdigest()\n",
    "    asint = int(hashcode,16)\n",
    "    #binstring = format(asint,'0128b')\n",
    "    #print(binstring)\n",
    "    current_lsb = least_significant_bit(asint)\n",
    "    ones_found[current_lsb]=\"1\"\n",
    "    max_lsb = max(max_lsb,current_lsb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the leftmost zero in our ones-place-holder for this dataset was 5, so our estimate of the cardinality is  41 \n"
     ]
    }
   ],
   "source": [
    "from hashlib import md5 \n",
    "\n",
    "def least_significant_bit(input_int):\n",
    "    \"\"\"\n",
    "    Return the least significant bit from an integer\n",
    "    With thanks to Mark Dickinson on SO: https://stackoverflow.com/questions/34166566/whats-the-fastest-method-to-return-the-position-of-the-least-significant-bit-se\n",
    "    \"\"\"\n",
    "    return  (input_int & -input_int).bit_length() - 1\n",
    "\n",
    "def hash_and_binarize(input_string):\n",
    "    \"\"\"\n",
    "    This takes our input strings and returns a string of ones and zeros for calculating the least significant bit\n",
    "    \"\"\"\n",
    "    m = md5()\n",
    "    m.update(input_string.encode())\n",
    "    hashcode = m.hexdigest()\n",
    "    asint = int(hashcode,16)\n",
    "    return asint\n",
    "\n",
    "max_lsb = -1\n",
    "ones_found = [\"0\" for i in range(128)]\n",
    "for element in jabber_string:\n",
    "    asint = hash_and_binarize(element)\n",
    "    #binstring = format(asint,'0128b')\n",
    "    #print(binstring)\n",
    "    current_lsb = least_significant_bit(asint)\n",
    "    ones_found[current_lsb]=\"1\"\n",
    "    max_lsb = max(max_lsb,current_lsb)\n",
    "    \n",
    "r_estimate = ones_found.index(\"0\")\n",
    "\n",
    "φ =0.77351\n",
    "λ = (1/φ)*2**r_estimate\n",
    "\n",
    "print(f\"the leftmost zero in our ones-place-holder for this dataset was {r_estimate}, so our estimate of the cardinality is  {int(λ)} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this was not an accurate enough measure in general, so he used the hash function to split the dataset up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 64062270616557689651147354933901131776\n",
      "3 52466348488662174541902960035912220672\n",
      "4 50607618690934308376383652158867767296\n",
      "4 38219638942521735058138015913826320384\n",
      "0 2982281716540308022146558625184743424\n",
      "1 64974960805370520535659059556500111360\n",
      "2 63410106522289915614136348768353648640\n",
      "2 57437846345176776457867036706757148672\n",
      "4 50607618690934308376383652158867767296\n",
      "2 53999122111702458886292074517032337408\n",
      "4 5239806017226476968630914916380835840\n",
      "4 38219638942521735058138015913826320384\n",
      "2 8349032152281506580856159421811654656\n",
      "4 42935776154101823344836562169075073024\n",
      "3 29617179120478454431849966109047390208\n",
      "4 11432843670683485205717818295235641344\n",
      "4 38219638942521735058138015913826320384\n",
      "1 43576542887033675643318322065779458048\n",
      "4 50607618690934308376383652158867767296\n",
      "4 38219638942521735058138015913826320384\n",
      "0 39920144810055736402360145151554224128\n",
      "2 50188371228337761670735655307717902336\n",
      "0 48039373802239570916549705365184315392\n",
      "74\n",
      "73\n",
      "73\n",
      "75\n",
      "70\n",
      "73\n",
      "0\n",
      "73\n",
      "74\n",
      "0\n",
      "75\n",
      "73\n",
      "75\n",
      "76\n",
      "73\n",
      "77\n",
      "71\n",
      "73\n",
      "75\n",
      "73\n",
      "0\n",
      "70\n",
      "72\n",
      "73\n",
      "0\n",
      "73\n",
      "74\n",
      "0\n",
      "the leftmost zero in our ones-place-holder for this dataset was 0.0, so our estimate of the cardinality is  1 \n"
     ]
    }
   ],
   "source": [
    "from hashlib import md5 \n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def least_significant_bit(input_int):\n",
    "    \"\"\"\n",
    "    Return the least significant bit from an integer\n",
    "    With thanks to Mark Dickinson on SO: https://stackoverflow.com/questions/34166566/whats-the-fastest-method-to-return-the-position-of-the-least-significant-bit-se\n",
    "    \"\"\"\n",
    "    return  (input_int & -input_int).bit_length() - 1\n",
    "\n",
    "def group_and_int(input_string, num_groups):\n",
    "    \"\"\"\n",
    "    This takes our input strings and returns a string of ones and zeros for calculating the least significant bit\n",
    "    \"\"\"\n",
    "    m = md5()\n",
    "    m.update(input_string.encode())\n",
    "    hashcode = m.hexdigest()\n",
    "    asint = int(hashcode,16)\n",
    "    alpha = asint % num_groups \n",
    "    rem_int = math.floor(asint/num_groups)\n",
    "    return (alpha, rem_int)\n",
    "\n",
    "num_batches = 5\n",
    "max_lsb = -1\n",
    "ones_found = [\"0\" for i in range(128)]\n",
    "group_dict = {}\n",
    "for element in jabber_string:\n",
    "    group, remaining_int = group_and_int(element, num_groups=num_batches)\n",
    "    print(group, remaining_int)\n",
    "    if group not in group_dict.keys():\n",
    "        group_dict[group] = [remaining_int]\n",
    "    else:\n",
    "        group_dict[group].append(remaining_int)\n",
    "    #current_lsb = least_significant_bit(asint)\n",
    "    #ones_found[current_lsb]=\"1\"\n",
    "    #max_lsb = max(max_lsb,current_lsb)\n",
    "    \n",
    "\n",
    "R_samples = []\n",
    "for group in group_dict.keys():\n",
    "    current_bitholder = [\"0\" for _ in range(128)]\n",
    "    for bin_string in group_dict[group]:\n",
    "        current_lsb = least_significant_bit(bin_string)\n",
    "        print(current_lsb)\n",
    "        current_bitholder[current_lsb]=\"1\"\n",
    "    sample_R = current_bitholder.index(\"0\")\n",
    "    print(sample_R)\n",
    "    R_samples.append(sample_R)\n",
    "\n",
    "\n",
    "R_estimate = (sum(R_samples))/num_batches\n",
    "φ =0.77351\n",
    "λ = (1/φ)*2**R_estimate\n",
    "\n",
    "print(f\"the leftmost zero in our ones-place-holder for this dataset was {R_estimate}, so our estimate of the cardinality is  {int(λ)} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "73\n",
      "73\n",
      "75\n",
      "70\n",
      "73\n",
      "0\n",
      "73\n",
      "74\n",
      "0\n",
      "75\n",
      "73\n",
      "75\n",
      "76\n",
      "73\n",
      "77\n",
      "71\n",
      "73\n",
      "75\n",
      "73\n",
      "0\n",
      "70\n",
      "72\n",
      "73\n",
      "0\n",
      "73\n",
      "74\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "R_samples = []\n",
    "for group in group_dict.keys():\n",
    "    current_bitholder = [\"0\" for _ in range(128)]\n",
    "    for bin_string in group_dict[group]:\n",
    "        current_lsb = least_significant_bit(bin_string)\n",
    "        print(current_lsb)\n",
    "        current_bitholder[current_lsb]=\"1\"\n",
    "    sample_R = current_bitholder.index(\"0\")\n",
    "    print(sample_R)\n",
    "    R_samples.append(sample_R)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-121-58ea362ec007>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-121-58ea362ec007>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://en.wikipedia.org/wiki/Flajolet–Martin_algorithm\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://en.wikipedia.org/wiki/Flajolet–Martin_algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Linear Counting?\n",
    "\n",
    "He also wrote Loglog (http://algo.inria.fr/flajolet/Publications/DuFl03-LNCS.pdf) (and superLogLog)\n",
    "\n",
    "Super Loglog is loglog + Truncation + Restriction\n",
    "\n",
    "Hyperloglog http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf Hyperloglog’s major innovation was switching from an arithmetic mean (v1 + v2 + v3)/n to a \n",
    "\n",
    "Google carried the torch. https://research.google.com/pubs/pub40671.html\n",
    "\n",
    "Finally, Hyperloglog TailCut and Tailcut+http://cse.seu.edu.cn/PersonalPage/csqjxiao/csqjxiao_files/papers/INFOCOM17-slides.pdf\n",
    "\n",
    "\n",
    "\n",
    "of the true sing a relatively smaller amount of memory to store a value that is only incremented probabilistically. The paper \n",
    "\n",
    "https://www.inf.ed.ac.uk/teaching/courses/exc/reading/morris.pdf\n",
    "\n",
    "Educational Resources:\n",
    "https://en.wikipedia.org/wiki/HyperLogLog\n",
    "http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf\n",
    "http://druid.io/blog/2014/02/18/hyperloglog-optimizations-for-real-world-systems.html \n",
    "\n",
    "\n",
    "## HyperLogLog:\n",
    "\n",
    "Hash the data and set the count the maximum number of leading zeroes as m. The estimate for the number of unique elements is 2*m. \n",
    "\n",
    "To reduce the variance of this estimator, they split the data into multiple sets, estimate the cardinality for each subset, and use the harmonic mean to come up with the hyperloglog estimate.\n",
    "\n",
    "\n",
    " In general, a COUNT (DISTINCT URL) requires memory space proportional to the number of unique values. HyperLogLog requires constant memory, runs on a stream (so you can add to it) over time without needing to record the actual data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
